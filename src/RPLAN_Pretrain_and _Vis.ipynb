{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "RPLAN_pretrain.ipynb",
   "provenance": [
    {
     "file_id": "1ObAYvVKQjMG5nd2wIno7j2y_X91E9IrX",
     "timestamp": 1647457248831
    }
   ],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jF8ZoVrwt0n0"
   },
   "source": [
    "# SimCLR on RPLAN Dataset\n",
    "In this notebook, we try to project the RPLAN dataset into embedding space by SimCLR to evaluate SimCLR's performance. \n",
    "\n",
    "RPLAN dataset is a manually collected large-scale densely annotated dataset of floor plans from real residential buildings.[From dataset discription]\n",
    "\n",
    "Here is an example in RPLAN dataset:\n",
    "\n",
    "<img src=\"http://staff.ustc.edu.cn/~fuxm/projects/DeepLayout/DeepLayout.png\" alt=\"examples\" width=\"400\"/>\n",
    "\n",
    "[Link to SimCLR](https://arxiv.org/pdf/2002.05709.pdf)\n",
    "\n",
    "[Link to RPLAN dataset](http://staff.ustc.edu.cn/~fuxm/projects/DeepLayout/index.html)\n",
    "\n",
    "### The main work can be divided into two parts:\n",
    "\n",
    "1. Change the binary images of RPLAN dataset to color images for better visualization\n",
    "2. Use the color images to pre-train SimCLR model.\n",
    "3. Use the pre-trained SimCLR model to project the RPLAN dataset into embedding space.\n",
    "4. Use tensorboard to visualize the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lt6WMxjCvN3o"
   },
   "source": [
    "## Setup the repository"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "53JMIYtat8tT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649346404875,
     "user_tz": -120,
     "elapsed": 11009,
     "user": {
      "displayName": "MT AB",
      "userId": "12376781480150994694"
     }
    },
    "outputId": "8c8323b4-da7f-4fd4-ff49-bceab7ba02f9",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "!git clone https://github.com/spijkervet/SimCLR.git\n",
    "%cd SimCLR\n",
    "!mkdir -p logs && cd logs && wget https://github.com/Spijkervet/SimCLR/releases/download/1.2/checkpoint_100.tar && cd ../\n",
    "!sh setup.sh || python3 -m pip install -r requirements.txt || exit 1\n",
    "!pip install  pyyaml --upgrade\n",
    "!pip install rplanpy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part1:\n",
    "## Proprocessing the RPLAN dataset\n",
    "In this part, we use rplanpy libary to read the RPLAN dataset and convert the binary images to color images. Here is two examples of this processing.\n",
    "\n",
    "<img src=\"https://cdn.discordapp.com/attachments/884910103428476989/961703864225112074/unknown.png\" alt=\"examples\" width=\"400\"/>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import rplanpy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import required module\n",
    "import os\n",
    "# assign directory\n",
    "directory = 'rplan_dataset' # The place you store the dataset\n",
    "number_of_images = 10000\n",
    "# iterate over files in\n",
    "# that directory\n",
    "file_list = []\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "      file_list.append(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for filename in file_list[0:number_of_images]:\n",
    "    cnt += 1\n",
    "    if cnt%100==0:\n",
    "      print('Processing image %d'%cnt)\n",
    "    temp_1 = rplanpy.plot.floorplan_to_color(rplanpy.data.RplanData(filename))\n",
    "    im = Image.fromarray(np.asarray(temp_1).astype(np.uint8))\n",
    "    im.save(\"images/\"+str(cnt)+\".png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQ3jq3cWynLf"
   },
   "source": [
    "# Part 2:\n",
    "## SimCLR pre-training\n",
    "\n",
    "In this part, we pre-train the SimCLR on the color images. The model we choose is ResNet18. And we just use the randomly initialized weights instead of pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import necessary libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oNDRcPbbymlX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649346408486,
     "user_tz": -120,
     "elapsed": 3,
     "user": {
      "displayName": "MT AB",
      "userId": "12376781480150994694"
     }
    },
    "outputId": "cf166fa9-11c1-4a14-927c-15290983f468"
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np  \n",
    "import torchvision\n",
    "import argparse\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "apex = False\n",
    "try:\n",
    "    from apex import amp\n",
    "    apex = True\n",
    "except ImportError:\n",
    "    print(\n",
    "        \"Install the apex package from https://www.github.com/nvidia/apex to use fp16 for training\"\n",
    "    )\n",
    "from model import save_model, load_optimizer\n",
    "from simclr import SimCLR\n",
    "from simclr.modules import get_resnet, NT_Xent\n",
    "from simclr.modules.transformations import TransformsSimCLR"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Install the apex package from https://www.github.com/nvidia/apex to use fp16 for training\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYbV0fa_y03Z"
   },
   "source": [
    "### Load arguments from `config/config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1klUf-IuyxdL"
   },
   "source": [
    "from pprint import pprint\n",
    "import argparse\n",
    "from utils import yaml_config_hook\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SimCLR\")\n",
    "config = yaml_config_hook(\"./config/config.yaml\")\n",
    "for k, v in config.items():\n",
    "    parser.add_argument(f\"--{k}\", default=v, type=type(v))\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O86__UhA0Lvr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649346414652,
     "user_tz": -120,
     "elapsed": 470,
     "user": {
      "displayName": "MT AB",
      "userId": "12376781480150994694"
     }
    },
    "outputId": "c0d62503-cd4d-42d4-9e20-2a32b89bff6b"
   },
   "source": [
    "### override any configuration parameters here, e.g. to adjust for use on GPUs on the Colab platform:\n",
    "args.batch_size = 128\n",
    "args.resnet = \"resnet18\"\n",
    "pprint(vars(args))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'batch_size': 128,\n",
      " 'dataparallel': 0,\n",
      " 'dataset': 'CIFAR10',\n",
      " 'dataset_dir': './datasets',\n",
      " 'device': device(type='cuda'),\n",
      " 'epoch_num': 100,\n",
      " 'epochs': 100,\n",
      " 'gpus': 1,\n",
      " 'image_size': 224,\n",
      " 'logistic_batch_size': 256,\n",
      " 'logistic_epochs': 500,\n",
      " 'model_path': 'save',\n",
      " 'nodes': 1,\n",
      " 'nr': 0,\n",
      " 'optimizer': 'Adam',\n",
      " 'pretrain': True,\n",
      " 'projection_dim': 64,\n",
      " 'reload': False,\n",
      " 'resnet': 'resnet18',\n",
      " 'seed': 42,\n",
      " 'start_epoch': 0,\n",
      " 'temperature': 0.5,\n",
      " 'weight_decay': 1e-06,\n",
      " 'workers': 8}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0Orbc2M1exV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649346419753,
     "user_tz": -120,
     "elapsed": 2498,
     "user": {
      "displayName": "MT AB",
      "userId": "12376781480150994694"
     }
    },
    "outputId": "48d71d2f-ad12-4b0c-ea32-9e7d6deefb2d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cd /content/drive/MyDrive/"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6s2Me5tM512l",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649346420609,
     "user_tz": -120,
     "elapsed": 4,
     "user": {
      "displayName": "MT AB",
      "userId": "12376781480150994694"
     }
    },
    "outputId": "230efeb7-553a-44c1-a494-872a98fbbf8d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive/test_set_rplan\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "import random"
   ],
   "metadata": {
    "id": "qA5rjiAq1U8I"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJfeOM9PzNoF"
   },
   "source": [
    "### Load dataset into train loader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YGcskdBsytbj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649347208877,
     "user_tz": -120,
     "elapsed": 525,
     "user": {
      "displayName": "MT AB",
      "userId": "12376781480150994694"
     }
    },
    "outputId": "8b9c344e-e551-41aa-86a6-70156f460350"
   },
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "train_dataset = datasets.ImageFolder(root=\"/content/drive/MyDrive/test_set_rplan/\",transform=TransformsSimCLR(size=args.image_size))\n",
    "print(len(train_dataset))\n",
    "if args.nodes > 1:\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_dataset, num_replicas=args.world_size, rank=rank, shuffle=True\n",
    "    )\n",
    "else:\n",
    "    train_sampler = None\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=(train_sampler is None),\n",
    "    drop_last=True,\n",
    "    num_workers=args.workers,\n",
    "    sampler=train_sampler,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5250\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBlXZwvjzPmp"
   },
   "source": [
    "### Load the SimCLR model, optimizer and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xERq_yHSzJRX"
   },
   "source": [
    "# initialize ResNet\n",
    "encoder = get_resnet(args.resnet, pretrained=False)\n",
    "n_features = encoder.fc.in_features  # get dimensions of fc layer\n",
    "\n",
    "# initialize model\n",
    "model = SimCLR(encoder, args.projection_dim, n_features)\n",
    "#model.load_state_dict(torch.load(\"/content/drive/MyDrive/checkpoint_100.tar\", map_location=args.device.type))\n",
    "\n",
    "model = model.to(args.device)\n",
    "\n",
    "# optimizer / loss\n",
    "optimizer, scheduler = load_optimizer(args, model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtNCVEynzjtV"
   },
   "source": [
    "### Initialize the criterion (NT-Xent loss)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u067AY93zh-k"
   },
   "source": [
    "criterion = NT_Xent(args.batch_size, args.temperature, world_size=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyJ3ulWqzViL"
   },
   "source": [
    "### Setup TensorBoard for logging experiments"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zZNieMqfzU7H"
   },
   "source": [
    "writer = SummaryWriter()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXMOVfg47Hlh"
   },
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KyMFhpB-7HGj"
   },
   "source": [
    "def train(args, train_loader, model, criterion, optimizer, writer):\n",
    "    loss_epoch = 0\n",
    "    for step, ((x_i, x_j), _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x_i = x_i.cuda(non_blocking=True)\n",
    "        x_j = x_j.cuda(non_blocking=True)\n",
    "\n",
    "        # positive pair, with encoding\n",
    "        h_i, h_j, z_i, z_j = model(x_i, x_j)\n",
    "        loss = criterion(z_i, z_j)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step [{step}/{len(train_loader)}]\\t Loss: {loss.item()}\")\n",
    "\n",
    "        writer.add_scalar(\"Loss/train_epoch\", loss.item(), args.global_step)\n",
    "        loss_epoch += loss.item()\n",
    "        args.global_step += 1\n",
    "    return loss_epoch\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cN5KBK-yztGD"
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TdCrD62hzjDQ"
   },
   "source": [
    "args.global_step = 0\n",
    "args.current_epoch = 0\n",
    "print(len(train_loader))\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    loss_epoch = train(args, train_loader, model, criterion, optimizer, writer)\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "    # save every 10 epochs\n",
    "    '''\n",
    "    if epoch % 10 == 0:\n",
    "        save_model(args, model, optimizer)\n",
    "    '''\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", loss_epoch / len(train_loader), epoch)\n",
    "    writer.add_scalar(\"Misc/learning_rate\", lr, epoch)\n",
    "    print(\n",
    "        f\"Epoch [{epoch}/{args.epochs}]\\t Loss: {loss_epoch / len(train_loader)}\\t lr: {round(lr, 5)}\"\n",
    "    )\n",
    "    args.current_epoch += 1\n",
    "\n",
    "# end training\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the checkpoints"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "args.model_path = \"/content/drive/MyDrive/rplan\"\n",
    "save_model(args, model, optimizer)"
   ],
   "metadata": {
    "id": "T648YZqPDMr_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAQpjiuJy61N"
   },
   "source": [
    "# Part 3:\n",
    "## Project the train set to the embedding space.\n",
    "\n",
    "Here, we only project the train set to the embedding space to save some time. (Because some errors in google drive, we only load 5000+ images into the train set.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kFyS9RvpuCuC"
   },
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import argparse"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the training set with no shuffle."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = datasets.ImageFolder(root=\"/content/drive/MyDrive/test_set_rplan/\",transform=TransformsSimCLR(size=args.image_size).test_transform)\n",
    "print(len(train_dataset))\n",
    "if args.nodes > 1:\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_dataset, num_replicas=args.world_size, rank=rank, shuffle=True\n",
    "    )\n",
    "else:\n",
    "    train_sampler = None\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=None,\n",
    "    drop_last=True,\n",
    "    num_workers=args.workers,\n",
    "    sampler=train_sampler,\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fCIOL--pwTJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649347358689,
     "user_tz": -120,
     "elapsed": 10123,
     "user": {
      "displayName": "MT AB",
      "userId": "12376781480150994694"
     }
    },
    "outputId": "dbf28744-75c8-49c2-a358-2c5064d54570"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5250\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OJk4-nc-vkF0"
   },
   "source": [
    "from pprint import pprint\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWRuVrZZ5Vm1"
   },
   "source": [
    "### Load dataset into train/test dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLgDCu1uTLQ5"
   },
   "source": [
    "### Helper functions to map all input data $X$ to their latent representations $h$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6B6li5NVSWR3"
   },
   "source": [
    "def inference(loader, simclr_model, device):\n",
    "    feature_vector = []\n",
    "    labels_vector = []\n",
    "    for step, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "\n",
    "        # get encoding\n",
    "        with torch.no_grad():\n",
    "            h = simclr_model(x)\n",
    "\n",
    "        h = h.detach()\n",
    "\n",
    "        feature_vector.extend(h.cpu().detach().numpy())\n",
    "        labels_vector.extend(y.numpy())\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print(f\"Step [{step}/{len(loader)}]\\t Computing features...\")\n",
    "\n",
    "    feature_vector = np.array(feature_vector)\n",
    "    labels_vector = np.array(labels_vector)\n",
    "    print(\"Features shape {}\".format(feature_vector.shape))\n",
    "    return feature_vector, labels_vector\n",
    "\n",
    "\n",
    "def get_features(context_model, train_loader, device):\n",
    "    train_X, train_y = inference(train_loader, context_model, device)\n",
    "    #test_X, test_y = inference(test_loader, context_model, device)\n",
    "    return train_X, train_y#test_X, test_y\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sPeoK6ZkS4MB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9ee2b3bc-ec5f-4b83-ac34-f8c50a59c8e7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649347415723,
     "user_tz": -120,
     "elapsed": 15943,
     "user": {
      "displayName": "MT AB",
      "userId": "12376781480150994694"
     }
    }
   },
   "source": [
    "print(\"### Creating features from pre-trained context model ###\")\n",
    "(train_X, train_y) = get_features(\n",
    "    encoder, train_loader, args.device\n",
    ")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### Creating features from pre-trained context model ###\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step [0/41]\t Computing features...\n",
      "Step [20/41]\t Computing features...\n",
      "Step [40/41]\t Computing features...\n",
      "Features shape (5248, 512)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 4:\n",
    "## Data Visualization\n",
    "\n",
    "In this part, we will visualize the embedding space of the train set. We refer to the blog [How to visualize image feature vectors](https://hanna-shares.medium.com/how-to-visualize-image-feature-vectors-1e309d45f28f). And we will use tensorboard visualize the embedding space."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dxK5MuRbR7tW"
   },
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import csv"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# transform the tensor to PIL image\n",
    "trans = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize([56, 56])                           \n",
    "  ]\n",
    ")\n",
    "DatasetPIL = []\n",
    "for (images,_) in train_loader:\n",
    "  for image in images:\n",
    "    DatasetPIL.append(trans(image))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49vkY_u1N5mE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649347451360,
     "user_tz": -120,
     "elapsed": 21417,
     "user": {
      "displayName": "MT AB",
      "userId": "12376781480150994694"
     }
    },
    "outputId": "30147a59-b93c-45b0-96a2-ce1e96f8eacf"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use the sprite library to aggregate all images into 1 images\n",
    "![](https://media.discordapp.net/attachments/884910103428476989/961711807645507684/unknown.png?width=1296&height=1302)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "image_number = 3000\n",
    "images = DatasetPIL[:3000]\n",
    "\n",
    "image_width, image_height = images[0].size\n",
    "one_square_size = int(np.ceil(np.sqrt(len(images))))\n",
    "master_width = (image_width * one_square_size) \n",
    "master_height = image_height * one_square_size\n",
    "spriteimage = Image.new(\n",
    "    mode='RGBA',\n",
    "    size=(master_width, master_height),\n",
    "    color=(0,0,0,0))  # fully transparent\n",
    "for count, image in enumerate(images):\n",
    "    div, mod = divmod(count,one_square_size)\n",
    "    h_loc = image_width*div\n",
    "    w_loc = image_width*mod    \n",
    "    spriteimage.paste(image,(w_loc,h_loc))\n",
    "spriteimage.convert(\"RGB\").save('vis/sprite56.jpg', transparency=0)"
   ],
   "metadata": {
    "id": "REU0iaFcNnlA",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Write all the embeddings into one csv file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "vecs = [vec for vec in train_X[:image_number]]\n",
    "with open('vis/feature_vecs56.tsv', 'w+') as fw:\n",
    "    csv_writer = csv.writer(fw, delimiter='\\t')\n",
    "    csv_writer.writerows(vecs)"
   ],
   "metadata": {
    "id": "P5HZsQVzPqDr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# we will also create the config file for tensorboard to identify the tensor and image\n",
    "`embeddings {\n",
    "  tensor_path: \"feature_vecs.tsv\"\n",
    "  metadata_path: \"metadata.tsv\"\n",
    "  sprite {\n",
    "    image_path: \"sprite.jpg\"\n",
    "    single_image_dim: 50\n",
    "    single_image_dim: 50\n",
    "  }\n",
    "}`\n",
    "# then run tensorboard\n",
    "`tensorboard --logdir ./vis`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}